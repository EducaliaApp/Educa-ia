# Pipeline ETL Completo - Flujo End-to-End

## Arquitectura del Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE ETL COMPLETO                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EXTRACT    ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  TRANSFORM   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ     LOAD     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                      ‚îÇ                      ‚îÇ
      ‚ñº                      ‚ñº                      ‚ñº
  Descargar PDF        Extraer Texto         Guardar en BD
  desde URLs           Generar Embeddings    PostgreSQL + pg_vector
```

## Componentes del Sistema

### 1. Edge Function: Monitor de Documentos
**Archivo**: `supabase/functions/monitor-documentos-oficiales/index.ts`

**Responsabilidades**:
- ‚úÖ Scrapear sitio DocenteM√°s
- ‚úÖ Detectar documentos nuevos/actualizados
- ‚úÖ Descargar PDFs y guardar en Supabase Storage
- ‚úÖ Registrar metadata en tabla `documentos_oficiales`
- ‚úÖ Marcar documentos como `procesado: false`

**Salida**:
```json
{
  "documentos_nuevos": 5,
  "documentos_actualizados": 2,
  "detalles": [...]
}
```

### 2. Script Python: Pipeline Completo ETL
**Archivo**: `scripts/pipeline-document-mineduc/pipeline-completo.py`

**Responsabilidades**:
- ‚úÖ Obtener documentos pendientes (`procesado: false`)
- ‚úÖ Descargar PDF desde `url_original`
- ‚úÖ Extraer texto con PyMuPDF
- ‚úÖ Generar embedding con OpenAI
- ‚úÖ Guardar texto + embedding en BD
- ‚úÖ Marcar como `procesado: true`

**Flujo Detallado**:

```python
# PASO 1: Obtener documentos pendientes
documentos = supabase.table('documentos_oficiales')
    .select('id, titulo, url_original')
    .eq('procesado', False)
    .execute()

# PASO 2: Procesar cada documento
for doc in documentos:
    # 2.1 Descargar PDF
    pdf_data = requests.get(doc['url_original']).content
    
    # 2.2 Extraer texto
    with fitz.open(stream=pdf_data) as pdf:
        texto = "\n".join([page.get_text() for page in pdf])
    
    # 2.3 Generar embedding
    embedding = openai.embeddings.create(
        model="text-embedding-3-small",
        input=texto[:8000]
    ).data[0].embedding
    
    # 2.4 Guardar en BD
    supabase.table('documentos_oficiales').update({
        'contenido_texto': texto,
        'embedding': embedding,
        'procesado': True,
        'fecha_procesamiento': datetime.now()
    }).eq('id', doc['id']).execute()
```

### 3. GitHub Actions Workflow
**Archivo**: `.github/workflows/sync-rubricas-mineduc.yml`

**Ejecuci√≥n**:
- üïê Autom√°tica: Domingos a las 2 AM UTC
- üîò Manual: Workflow dispatch

**Jobs**:

```yaml
1. monitor-documentos
   ‚îú‚îÄ Invocar Edge Function
   ‚îú‚îÄ Detectar documentos nuevos
   ‚îî‚îÄ Output: has_changes=true/false

2. process-documents (si has_changes=true)
   ‚îú‚îÄ Ejecutar pipeline-completo.py
   ‚îú‚îÄ Descargar + Procesar + Guardar
   ‚îî‚îÄ Output: processed_count=N

3. extract-rubricas (si processed_count>0)
   ‚îú‚îÄ Ejecutar rubric-extractor.py
   ‚îú‚îÄ Extraer r√∫bricas estructuradas
   ‚îî‚îÄ Guardar en tabla rubricas_mbe
```

## Flujo Completo End-to-End

### Escenario: Nuevo Documento Detectado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. DETECCI√ìN (Edge Function)                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

DocenteM√°s Website
    ‚îÇ
    ‚îú‚îÄ> Scraping HTML
    ‚îÇ   ‚îî‚îÄ> Detectar: "Manual Ed. B√°sica 2025.pdf"
    ‚îÇ
    ‚îú‚îÄ> Descargar PDF
    ‚îÇ   ‚îî‚îÄ> Guardar en Storage: documentos-oficiales/manuales/2025/...
    ‚îÇ
    ‚îî‚îÄ> Registrar en BD
        INSERT INTO documentos_oficiales (
            titulo: "Manual Ed. B√°sica 2025",
            url_original: "https://...",
            storage_path: "manuales/2025/...",
            procesado: FALSE  ‚Üê IMPORTANTE
        )

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. PROCESAMIENTO (Python Pipeline)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

SELECT * FROM documentos_oficiales WHERE procesado = FALSE
    ‚îÇ
    ‚îú‚îÄ> Documento encontrado: "Manual Ed. B√°sica 2025"
    ‚îÇ
    ‚îú‚îÄ> EXTRACT: Descargar desde url_original
    ‚îÇ   ‚îî‚îÄ> requests.get(url_original) ‚Üí pdf_data (bytes)
    ‚îÇ
    ‚îú‚îÄ> TRANSFORM: Procesar PDF
    ‚îÇ   ‚îú‚îÄ> PyMuPDF: Extraer texto ‚Üí "Objetivos de aprendizaje..."
    ‚îÇ   ‚îî‚îÄ> OpenAI API: Generar embedding ‚Üí [0.123, -0.456, ...] (1536 dims)
    ‚îÇ       ‚ö†Ô∏è OpenAI NO almacena el embedding, solo lo genera
    ‚îÇ
    ‚îî‚îÄ> LOAD: Guardar en PostgreSQL con pg_vector
        UPDATE documentos_oficiales SET
            contenido_texto = "Objetivos de aprendizaje...",
            embedding = [0.123, -0.456, ...],  ‚Üê Vector guardado en PostgreSQL
            procesado = TRUE,
            fecha_procesamiento = NOW(),
            embedding_model = 'text-embedding-3-small',
            embedding_version = 'v1.0'
        WHERE id = documento_id
        
        ‚úÖ Embedding persistido en PostgreSQL para b√∫squedas futuras

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. EXTRACCI√ìN DE R√öBRICAS (Opcional)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Si tipo_documento = 'rubricas':
    ‚îÇ
    ‚îú‚îÄ> Leer contenido_texto
    ‚îÇ
    ‚îú‚îÄ> Extraer con IA (OpenAI ‚Üí Gemini ‚Üí Cohere ‚Üí Anthropic)
    ‚îÇ   ‚îî‚îÄ> JSON estructurado con criterios y niveles
    ‚îÇ
    ‚îî‚îÄ> Guardar en tabla rubricas_mbe
        INSERT INTO rubricas_mbe (
            documento_id,
            asignatura,
            nivel_educativo,
            criterios: {...},
            niveles_desempe√±o: {...}
        )
```

## Ventajas del Dise√±o Actual

### ‚úÖ Separaci√≥n de Responsabilidades
- **Edge Function**: Detecci√≥n y descarga inicial
- **Python Script**: Procesamiento pesado (OCR, embeddings)
- **Rubric Extractor**: Extracci√≥n estructurada con IA

### ‚úÖ Resiliencia
- Cada componente puede fallar independientemente
- Reintentos autom√°ticos en cada etapa
- Estado persistente en BD (`procesado: false/true`)

### ‚úÖ Escalabilidad
- Procesamiento por lotes (50 documentos a la vez)
- L√≠mite de p√°ginas (50 por PDF)
- Timeout protection en cada etapa

### ‚úÖ Observabilidad
- Logs detallados en cada paso
- M√©tricas en tabla `metricas_pipeline_rag`
- Notificaciones a administradores

## Optimizaciones Implementadas

### 1. Sin Almacenamiento Redundante
```python
# ‚ùå ANTES: Descargar desde Storage
pdf_data = supabase.storage.from_('documentos-oficiales').download(storage_path)

# ‚úÖ AHORA: Descargar desde URL original
pdf_data = requests.get(url_original).content
```

**Ahorro**: ~90% en costos de Storage

### 2. Procesamiento en Memoria
```python
# Todo el procesamiento en memoria, sin archivos temporales
with fitz.open(stream=pdf_data, filetype="pdf") as doc:
    texto = extraer_texto(doc)
    embedding = generar_embedding(texto)
    guardar_en_bd(texto, embedding)
```

### 3. Embeddings Optimizados
```python
# Usar solo contenido relevante (primeras 8000 caracteres)
texto_limpio = limpiar_texto(texto)[:8000]
embedding = openai.embeddings.create(
    model="text-embedding-3-small",  # Modelo m√°s econ√≥mico
    input=texto_limpio
)
```

**Ahorro**: ~$0.01 por documento

## Monitoreo y Debugging

### Ver Documentos Pendientes
```sql
SELECT id, titulo, url_original, fecha_descarga
FROM documentos_oficiales
WHERE procesado = FALSE
ORDER BY fecha_descarga DESC;
```

### Ver Documentos Procesados Hoy
```sql
SELECT id, titulo, 
       LENGTH(contenido_texto) as texto_length,
       embedding_model,
       fecha_procesamiento
FROM documentos_oficiales
WHERE procesado = TRUE
  AND fecha_procesamiento::date = CURRENT_DATE;
```

### Ver Errores de Procesamiento
```sql
SELECT documento_id, error_mensaje, fecha_error
FROM reintentos_procesamiento
WHERE fecha_error > NOW() - INTERVAL '24 hours'
ORDER BY fecha_error DESC;
```

## Ejecuci√≥n Manual

### Ejecutar Pipeline Completo
```bash
cd scripts/pipeline-document-mineduc
python pipeline-completo.py
```

### Ejecutar Solo Extracci√≥n de R√∫bricas
```bash
python rubric-extractor.py --auto --verbose
```

### Ejecutar Workflow Completo
```bash
# Desde GitHub Actions UI
# 1. Ir a Actions tab
# 2. Seleccionar "Sync Datos MINEDUC"
# 3. Click "Run workflow"
# 4. Configurar opciones:
#    - force_full_sync: true/false
#    - force_rubric_extraction: true/false
```

## Pr√≥ximos Pasos

### Mejoras Planificadas
- [ ] Procesamiento paralelo (m√∫ltiples documentos simult√°neos)
- [ ] Cache de embeddings para documentos similares
- [ ] Detecci√≥n inteligente de cambios (diff sem√°ntico)
- [ ] Compresi√≥n de texto antes de almacenar
- [ ] √çndices vectoriales optimizados (HNSW en lugar de IVFFlat)

### Integraciones Futuras
- [ ] Webhook para notificaciones en tiempo real
- [ ] Dashboard de monitoreo del pipeline
- [ ] API REST para consultar estado del pipeline
- [ ] Exportaci√≥n de m√©tricas a Prometheus/Grafana
