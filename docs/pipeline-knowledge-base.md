# Pipeline de Base de Conocimientos - ProfeFlow

## Misi√≥n del Pipeline

**Obtener TODOS los datos de DocenteM√°s y crear una base de conocimientos s√≥lida con embeddings en base de datos vectorial PostgreSQL.**

## Arquitectura de Datos

### 1. **Tabla Principal: `documentos_oficiales`**

Almacena documentos completos con metadata y embeddings:

```sql
- id (UUID): Identificador √∫nico
- titulo: Nombre del documento
- tipo_documento: rubrica | manual_portafolio | base_curricular | instructivo
- nivel_educativo: basica_1_6 | basica_7_8 | media | parvularia | especial | epja
- asignatura: matematica | lenguaje | ciencias_naturales | etc
- modalidad: regular | epja | especial_escuela | tecnico_profesional
- a√±o_vigencia: 2025
- contenido_texto: Texto completo extra√≠do
- embedding (vector): Embedding del documento completo
- hash_contenido: SHA-256 para deduplicaci√≥n
- procesado: true/false
- url_original: URL fuente
```

### 2. **Tabla de Chunks: `chunks_documentos`**

Divide documentos en fragmentos para RAG optimizado:

```sql
- id (UUID): Identificador √∫nico
- documento_id: Referencia a documentos_oficiales
- contenido: Texto del chunk
- chunk_index: Orden del chunk
- seccion: Secci√≥n del documento
- embedding (vector): Embedding del chunk
- metadata (JSONB): Informaci√≥n adicional
```

### 3. **Cache de Embeddings: `cache_embeddings`**

Optimiza consultas frecuentes:

```sql
- query_hash: Hash de la consulta
- query_text: Texto de la consulta
- embedding (vector): Embedding cacheado
- uso_count: Contador de uso
- last_used_at: √öltima vez usado
```

### 4. **M√©tricas: `metricas_pipeline_rag`**

Tracking de ejecuciones:

```sql
- fecha: Fecha de ejecuci√≥n
- documentos_monitoreados: Total encontrados
- documentos_procesados: Total procesados
- chunks_validados: Total de chunks
- errores_criticos: Errores encontrados
```

## Flujo del Pipeline

### Fase 1: Monitoreo (monitor-documentos)
```
DocenteM√°s URLs ‚Üí Scraping ‚Üí Clasificaci√≥n ‚Üí BD
```
- ‚úÖ R√∫bricas: `/documentos-descargables/rubricas/`
- ‚úÖ Manuales: `/documentos-descargables/manuales-de-instrumentos/`
- ‚úÖ Curriculares: `/documentos-descargables/documentos-curriculares/`

### Fase 2: Procesamiento (process-documents)
```
PDF ‚Üí Extracci√≥n Texto ‚Üí Limpieza ‚Üí documentos_oficiales
```
- ‚úÖ PyMuPDF para PDFs normales
- ‚úÖ Tesseract OCR para PDFs escaneados
- ‚úÖ Deduplicaci√≥n por hash
- ‚úÖ Sin almacenamiento en Storage (solo texto)

### Fase 3: Generaci√≥n de Embeddings
```
Texto ‚Üí OpenAI API ‚Üí Vector (1536 dims) ‚Üí pg_vector
```
- ‚úÖ Modelo: `text-embedding-3-small`
- ‚úÖ Almacenamiento: PostgreSQL con pg_vector
- ‚úÖ √çndice: IVFFlat para b√∫squeda r√°pida

### Fase 4: Chunking (Opcional)
```
Documento ‚Üí Chunks (500 tokens) ‚Üí chunks_documentos
```
- ‚úÖ Chunks con overlap para contexto
- ‚úÖ Embeddings por chunk
- ‚úÖ Metadata preservada

### Fase 5: Extracci√≥n de R√∫bricas
```
Texto ‚Üí AI (OpenAI/Gemini/Cohere/Anthropic) ‚Üí rubricas_mbe
```
- ‚úÖ Extracci√≥n estructurada con IA
- ‚úÖ 4-tier fallback para confiabilidad
- ‚úÖ Validaci√≥n de JSON

### Fase 6: Validaci√≥n
```
Datos ‚Üí Quality Checks ‚Üí M√©tricas
```
- ‚úÖ Validaci√≥n de embeddings
- ‚úÖ Quality score por documento
- ‚úÖ Detecci√≥n de errores cr√≠ticos

## √çndices Vectoriales

### IVFFlat en chunks_documentos
```sql
CREATE INDEX idx_chunks_embedding 
ON chunks_documentos 
USING ivfflat (embedding vector_cosine_ops);
```

### Full-Text Search
```sql
CREATE INDEX idx_chunks_fts 
ON chunks_documentos 
USING gin (to_tsvector('spanish', contenido));
```

## B√∫squeda H√≠brida

### 1. B√∫squeda Vectorial
```sql
SELECT * FROM chunks_documentos
ORDER BY embedding <=> query_embedding
LIMIT 10;
```

### 2. B√∫squeda por Texto
```sql
SELECT * FROM chunks_documentos
WHERE to_tsvector('spanish', contenido) @@ to_tsquery('spanish', 'planificaci√≥n');
```

### 3. B√∫squeda H√≠brida (Vector + Texto)
```sql
SELECT *, 
  (embedding <=> query_embedding) as vector_distance,
  ts_rank(to_tsvector('spanish', contenido), query) as text_rank
FROM chunks_documentos
WHERE to_tsvector('spanish', contenido) @@ query
ORDER BY (vector_distance * 0.7 + (1 - text_rank) * 0.3)
LIMIT 10;
```

## Optimizaciones de Costos

### 1. Sin Storage de PDFs
- ‚ùå No guardamos PDFs en Supabase Storage
- ‚úÖ Solo guardamos texto extra√≠do
- üí∞ Ahorro: ~90% en costos de storage

### 2. Cache de Embeddings
- ‚úÖ Consultas frecuentes cacheadas
- ‚úÖ Reduce llamadas a OpenAI API
- üí∞ Ahorro: ~50% en costos de embeddings

### 3. Procesamiento Semanal
- ‚úÖ Ejecuci√≥n autom√°tica domingos 2 AM
- ‚úÖ Solo procesa documentos nuevos/actualizados
- üí∞ Ahorro: Procesamiento incremental

## M√©tricas de Calidad

### Quality Score (0-100)
```python
score = 0
if len(contenido_texto) > 100: score += 40
if embedding is not None: score += 30
if nivel_educativo: score += 10
if asignatura: score += 10
if a√±o_vigencia: score += 10
```

### Criterios de Validaci√≥n
- ‚úÖ Texto extra√≠do > 100 caracteres
- ‚úÖ Embedding generado exitosamente
- ‚úÖ Metadata completa
- ‚úÖ Sin errores de procesamiento

## Monitoreo y Alertas

### M√©tricas Clave
- üìä Documentos procesados / Total
- üìä Embeddings generados / Total
- üìä Quality score promedio
- üìä Tasa de error
- üìä Costo por documento

### Alertas
- üö® Error rate > 10%
- üö® Quality score < 70
- üö® Documentos sin embedding
- üö® Fallos en APIs

## Uso de la Base de Conocimientos

### 1. RAG (Retrieval Augmented Generation)
```typescript
// Buscar contexto relevante
const chunks = await supabase
  .rpc('buscar_chunks_similares', {
    query_embedding: embedding,
    match_threshold: 0.8,
    match_count: 5
  });

// Generar respuesta con contexto
const response = await openai.chat.completions.create({
  messages: [
    { role: 'system', content: 'Eres un asistente educativo...' },
    { role: 'user', content: `Contexto: ${chunks}\n\nPregunta: ${question}` }
  ]
});
```

### 2. B√∫squeda Sem√°ntica
```typescript
// Buscar documentos similares
const docs = await supabase
  .from('documentos_oficiales')
  .select('*')
  .order('embedding <=> ' + query_embedding)
  .limit(10);
```

### 3. Filtrado por Metadata
```typescript
// Buscar por nivel y asignatura
const docs = await supabase
  .from('chunks_documentos')
  .select('*')
  .eq('nivel_educativo', 'basica_1_6')
  .eq('asignatura', 'matematica')
  .order('embedding <=> ' + query_embedding)
  .limit(5);
```

## Roadmap

### Completado ‚úÖ
- [x] Monitoreo autom√°tico de DocenteM√°s
- [x] Extracci√≥n de texto con OCR
- [x] Generaci√≥n de embeddings
- [x] Base de datos vectorial
- [x] Extracci√≥n de r√∫bricas con IA
- [x] Validaci√≥n de calidad
- [x] CI/CD con GitHub Actions

### En Progreso üîÑ
- [ ] Chunking autom√°tico de documentos
- [ ] Cache de embeddings implementado
- [ ] Dashboard de m√©tricas
- [ ] Alertas autom√°ticas

### Planificado üìã
- [ ] B√∫squeda h√≠brida optimizada
- [ ] Reranking de resultados
- [ ] Actualizaci√≥n incremental
- [ ] Versionado de documentos
- [ ] API p√∫blica de b√∫squeda

## Conclusi√≥n

El pipeline cumple con la misi√≥n de:
1. ‚úÖ Obtener TODOS los datos de DocenteM√°s
2. ‚úÖ Procesar y extraer texto completo
3. ‚úÖ Generar embeddings con OpenAI
4. ‚úÖ Almacenar en base de datos vectorial (pg_vector)
5. ‚úÖ Crear base de conocimientos s√≥lida y consultable
6. ‚úÖ Optimizar costos (sin storage de PDFs)
7. ‚úÖ Garantizar calidad con validaciones
8. ‚úÖ Ejecutar autom√°ticamente cada semana

La base de conocimientos est√° lista para ser usada en aplicaciones RAG, b√∫squeda sem√°ntica, y asistentes educativos inteligentes.
